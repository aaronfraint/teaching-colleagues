{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied `pandas` examples: reading, joining, filtering, aggregating, and exporting\n",
    "\n",
    "This cell was changed from `code` to `markdown` at the top of the page. It lets me write nicely-formatted text that does not get executed like code.\n",
    "\n",
    "[Markdown is a stripped-down method for making simple text look nice. This is a link that will take you to GitHub's guide on the topic](https://guides.github.com/features/mastering-markdown/)\n",
    "\n",
    "A few notes about file/folder paths in Python:\n",
    "- Mac OS uses forward slashes, like `\"/path/to/somewhere\"`\n",
    "- Windows uses back slashes and a drive name, like `\"drive:\\path\\to\\somewhere\"`\n",
    "- Since backslashes have a special meaning in Python you have two choices on Windows:\n",
    "    - Double up your slashes, like `\"drive:\\\\path\\\\to\\\\somewhere\"`\n",
    "    - Or, put the letter `r` in front of the string, like `r\"drive:\\path\\to\\somewhere\"`\n",
    "    - Using `r` makes life easier than doubling up the slashes as you can copy/paste folder and filepaths and use them as-is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_folder_on_macos = \"/Volumes/GoogleDrive/Shared drives/Long Range Plan/Models/UrbanSim/Model Development/Base Year Migration/FinalAllocation20201006/fromSol\"\n",
    "data_folder_on_windows = r\"G:\\Shared drives\\Long Range Plan\\Models\\UrbanSim\\Model Development\\Base Year Migration\\FinalAllocation20201006\\fromSol\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am going to use `data_folder_on_macos`, but when you run this on your machine you should swap in `data_folder_on_windows`\n",
    "\n",
    "Just remove the pound sign and single space in front of the second line in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = data_folder_on_macos\n",
    "# data_folder = data_folder_on_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a variety of ways to list the contents of a folder.\n",
    "\n",
    "`os` and `pathlib` are two different built-in Python modules we can use. For simplicity I'll use `os`, which does \"operating system\" stuff. Also, note that I'm importing it a few cells deep into the notebook. This works fine, but is a violation of a best-practice, which is to do all of your imports up front in the first set of lines in your script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "csvs_in_folder = os.listdir(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allocated_hh_34_005_2017_adj.csv',\n",
       " 'allocated_p_34_005_2017_adj.csv',\n",
       " 'allocated_hh_34_007_2017_adj.csv',\n",
       " 'allocated_p_34_007_2017_adj.csv',\n",
       " 'allocated_hh_34_015_2017_adj.csv',\n",
       " 'allocated_p_34_015_2017_adj.csv',\n",
       " 'allocated_hh_34_021_2017_adj.csv',\n",
       " 'allocated_p_34_021_2017_adj.csv',\n",
       " 'allocated_hh_42_017_2017_adj.csv',\n",
       " 'allocated_p_42_017_2017_adj.csv',\n",
       " 'allocated_hh_42_029_2017_adj.csv',\n",
       " 'allocated_p_42_029_2017_adj.csv',\n",
       " 'allocated_hh_42_045_2017_adj.csv',\n",
       " 'allocated_p_42_045_2017_adj.csv',\n",
       " 'allocated_hh_42_091_2017_adj.csv',\n",
       " 'allocated_p_42_091_2017_adj.csv',\n",
       " 'allocated_hh_42_101_2017_adj.csv',\n",
       " 'allocated_p_42_101_2017_adj.csv',\n",
       " 'adjusted_units_20201006.csv',\n",
       " 'aggregated outputs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also leave notes with the standard \"comment\" style in python, by prefixing a line with a pound sign\n",
    "\n",
    "# Once you've created a variable, you can execute a cell with that variable to inspect it.\n",
    "# Let's see all of the filenames in the folder:\n",
    "\n",
    "csvs_in_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's loop through the list of files and place them into an appropriate list, either for households or people\n",
    "\n",
    "household_files = []\n",
    "person_files = []\n",
    "\n",
    "for filename in csvs_in_folder:\n",
    "    if \"_hh_\" in filename:\n",
    "        household_files.append(filename)\n",
    "    elif \"_p_\" in filename:\n",
    "        person_files.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's read each household CSV into a dataframe and drop it into a list of all household dataframes\n",
    "\n",
    "hh_dataframes = []\n",
    "\n",
    "for hh_file in household_files:\n",
    "\n",
    "    # Build the full path to the file by joining the folder name to the filename\n",
    "    filepath = os.path.join(data_folder, hh_file)\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    hh_dataframes.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can merge a list of dataframes together using `pd.concat(my_list)`\n",
    "\n",
    "Make sure to set it equal to something so that you can use it later. I.E. `new_df = pd.concat([df1, df2, df3])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_data = pd.concat(hh_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To confirm it worked, we can check how many unique county IDs are in our new dataframe\n",
    "The syntax here is `dataframe[\"column_name\"].unique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   7,  15,  21,  17,  29,  45,  91, 101])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh_data[\"county\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's repeat the process for the person tables\n",
    "\n",
    "person_dataframes = []\n",
    "\n",
    "for p_file in person_files:\n",
    "\n",
    "    filepath = os.path.join(data_folder, p_file)\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    person_dataframes.append(df)\n",
    "    \n",
    "person_data = pd.concat(person_dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As an aside, you can define functions with inputs\n",
    "\n",
    "This is advisable anytime you have a chunk of code that you've repeated. This is known as \"DRY\", or \"Don't Repeat Yourself\".\n",
    "\n",
    "Functions start with `def` and usually `return` something. Here's how I could have done this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function definition\n",
    "\n",
    "def merge_list_of_csvs(folder, list_of_filenames):\n",
    "    all_dataframes = []\n",
    "    \n",
    "    for single_file in list_of_filenames:\n",
    "        filepath = os.path.join(folder, single_file)\n",
    "        df = pd.read_csv(filepath)\n",
    "        all_dataframes.append(df)\n",
    "        \n",
    "    merged_df = pd.concat(all_dataframes)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Now I'm using the function twice, once for person files and once for household files\n",
    "person_data = merge_list_of_csvs(data_folder, person_files)\n",
    "hh_data = merge_list_of_csvs(data_folder, household_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you're curious about how many rows/columns are in a dataframe:\n",
    "\n",
    "Use `df.shape` to output `(row count, column count)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5547041, 14)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this case, we have over 5.5 million rows and 14 columns. Definitely too big for Excel!\n",
    "\n",
    "person_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining dataframes together\n",
    "\n",
    "Both household and person tables share a `\"household_id\"` column that we'll use to join them.\n",
    "\n",
    "The result will be a new dataframe that has a row for each person and the household's info appended in additional columns.\n",
    "\n",
    "`Pandas` is kind of tricky from a language point of view. You'd think `pd.join()` would be the right function here, but we actually want to use `pd.merge()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.merge(person_data, hh_data, on=\"household_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's confirm the table size so we can be sure it worked properly\n",
    "\n",
    "`combined_df` should have the same number of rows as the starting `person_data` table.\n",
    "\n",
    "`df.shape[0]` lets you grab the row count. I'll write a formula that will tell us if they match using `True` or `False`.\n",
    "\n",
    "The double equal sign `==` lets us test if two things are the same.\n",
    "\n",
    "This is a nice way to build QAQC directly into your script and ensure that your output matches what you'd expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape[0] == person_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do a basic aggregation\n",
    "\n",
    "We'll start by getting the total number of people by block group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "block_id\n",
       "340057001021001    47\n",
       "340057001021002    23\n",
       "340057001021003    40\n",
       "340057001021004    46\n",
       "340057001021005    34\n",
       "                   ..\n",
       "421019809001173    12\n",
       "421019809001203     9\n",
       "421019809001204     1\n",
       "421019809001205     7\n",
       "421019809001206     8\n",
       "Length: 72643, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The code below groups on the \"block_id\" column and the .size() bit counts the number of rows in each group\n",
    "\n",
    "combined_df.groupby([\"block_id\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The output of `df.group_by()` is a `pandas.Series`, which is equivalent to a column. \n",
    "\n",
    "We'll drop this series into a new dataframe and then rename the 'count' column, which by default will be named `0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to save this to a new dataframe, we'll set it equal to a new variable and drop it into pd.DataFrame()\n",
    "\n",
    "block_id_series = combined_df.groupby([\"block_id\"]).size()\n",
    "\n",
    "block_id_df = pd.DataFrame(block_id_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column \"0\" as \"total_people\". The \"inplace=True\" argument modifies the dataframe and means we don't need to assign it to a new variable\n",
    "\n",
    "rename_dictionary = {0: \"total_people\"}\n",
    "\n",
    "block_id_df.rename(columns=rename_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_people</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>block_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340057001021001</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340057001021002</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340057001021003</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340057001021004</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340057001021005</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421019809001173</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421019809001203</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421019809001204</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421019809001205</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421019809001206</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72643 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 total_people\n",
       "block_id                     \n",
       "340057001021001            47\n",
       "340057001021002            23\n",
       "340057001021003            40\n",
       "340057001021004            46\n",
       "340057001021005            34\n",
       "...                       ...\n",
       "421019809001173            12\n",
       "421019809001203             9\n",
       "421019809001204             1\n",
       "421019809001205             7\n",
       "421019809001206             8\n",
       "\n",
       "[72643 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_id_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's use some arbitrary age filters and then re-aggregate\n",
    "\n",
    "For this first example, we'll do everyone over the age of 50.\n",
    "\n",
    "The syntax is kind of weird here, but basically we get a true/false for each row if it meets the condition, and then all false rows are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_50_df = combined_df[combined_df[\"age\"] >= 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min age: 50\n",
      "Max age: 95\n"
     ]
    }
   ],
   "source": [
    "print(\"Min age:\", over_50_df[\"age\"].min())\n",
    "print(\"Max age:\", over_50_df[\"age\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaining multiple filters together is possible, but a bit more verbose.\n",
    "\n",
    "Note that we wrap each condition inside parentheses and use the `&` symbol to say that it must meet both conditions.\n",
    "\n",
    "This example filters to all people between 30 and 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30_to_50 = combined_df[(combined_df[\"age\"] >= 30) & (combined_df[\"age\"] < 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min age: 30\n",
      "Max age: 49\n"
     ]
    }
   ],
   "source": [
    "print(\"Min age:\", df_30_to_50[\"age\"].min())\n",
    "print(\"Max age:\", df_30_to_50[\"age\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two new dataframes, `over_50_df` and `df_30_to_50`.\n",
    "\n",
    "Each of these can be aggregated as before. I'll move forward with `df_30_to_50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_filtered_series = df_30_to_50.groupby([\"block_id\"]).size()\n",
    "\n",
    "age_filtered_df = pd.DataFrame(age_filtered_series)\n",
    "\n",
    "rename_dictionary = {0: \"people_30_thru_49\"}\n",
    "\n",
    "age_filtered_df.rename(columns=rename_dictionary, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We have what we need, now we want to get out of Python!\n",
    "\n",
    "All dataframes have the ability to be written out to file, but be aware of your row/column count. Excel maxes out at 1.04 million rows 16k columns.\n",
    "\n",
    "You're more likely to hit the row limit than the column limit. Either way, if you're close to the limit you'll definitely benefit from using SQL to store/access your data. More on that at a later date.\n",
    "\n",
    "To write to file, use `df.to_csv()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a filepath for the full aggregation and write to CSV\n",
    "\n",
    "output_filepath = os.path.join(data_folder, \"aggregated outputs\", \"block_id_agg_all_people.csv\")\n",
    "block_id_df.to_csv(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's repeat using the 30 to 50 filtered aggregation\n",
    "\n",
    "output_filepath = os.path.join(data_folder, \"aggregated outputs\", \"block_id_agg_30_to_50.csv\")\n",
    "age_filtered_df.to_csv(output_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
